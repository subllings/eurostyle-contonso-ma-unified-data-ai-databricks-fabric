{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e25df9",
   "metadata": {},
   "source": [
    "# Feature 3.2 — Feature Engineering (RFM, Basket Diversity, Cross‑brand)\n",
    "This notebook implements Feature 3.2: compute RFM and behavioral features, enforce anti‑leakage (as‑of date), run screening (correlation/cardinality), define preprocessing (imputation/log transforms fit on TRAIN only), persist versioned features, and log metadata to MLflow.\n",
    "\n",
    "Notes:\n",
    "- Synthetic fallback is included so this runs without data access; switch to Spark tables for real data.\n",
    "- Outputs: versioned features in artifacts (Parquet) and optional Delta write if Spark is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed119774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & setup\n",
    "import os, json, random, warnings, io\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style='whitegrid')\n",
    "try:\n",
    "    import mlflow\n",
    "except Exception:\n",
    "    mlflow = None\n",
    "\n",
    "SEED = int(os.environ.get('SEED', 42))\n",
    "np.random.seed(SEED); random.seed(SEED)\n",
    "AS_OF_DATE = pd.to_datetime(os.environ.get('AS_OF_DATE', '2024-12-31'))\n",
    "USE_SYNTHETIC = os.environ.get('USE_SYNTHETIC', 'true').lower() in ('1','true','yes')\n",
    "FEATURE_VERSION = os.environ.get('FEATURE_VERSION', 'v1')\n",
    "SOURCE_SNAPSHOT = os.environ.get('SOURCE_SNAPSHOT', datetime.utcnow().strftime('%Y-%m-%d'))\n",
    "ARTIFACT_DIR = 'artifacts/features'\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "print({'seed': SEED, 'as_of_date': str(AS_OF_DATE.date()), 'use_synthetic': USE_SYNTHETIC, 'feature_version': FEATURE_VERSION})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ff3f1",
   "metadata": {},
   "source": [
    "## Load data (synthetic fallback or Spark tables)\n",
    "Expected minimum columns:\n",
    "- customers: customer_id, brand\n",
    "- transactions: customer_id, tx_date, amount, cost, brand, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc37a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic(n_customers=1200, start='2023-01-01', end='2024-12-31'):\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    customers = pd.DataFrame({\n",
    "        'customer_id': np.arange(1, n_customers+1),\n",
    "        'brand': rng.choice(['Contoso','EuroStyle'], size=n_customers, p=[0.6,0.4])\n",
    "    })\n",
    "    dates = pd.date_range(start=start, end=end, freq='D')\n",
    "    cats = ['Shoes','Apparel','Accessories','Home','Beauty']\n",
    "    rows = []\n",
    "    for cid, brand in customers[['customer_id','brand']].itertuples(index=False):\n",
    "        k = rng.poisson(12)\n",
    "        if k == 0: continue\n",
    "        tx_days = np.sort(rng.choice(dates, size=k, replace=False))\n",
    "        for d in tx_days:\n",
    "            cat = rng.choice(cats)\n",
    "            base = 60 if brand=='Contoso' else 55\n",
    "            amount = float(np.round(rng.normal(base, 22), 2))\n",
    "            amount = max(5.0, amount)\n",
    "            cost = float(np.round(amount * rng.uniform(0.5, 0.8), 2))\n",
    "            rows.append((cid, pd.Timestamp(d), amount, cost, brand, cat))\n",
    "    tx = pd.DataFrame(rows, columns=['customer_id','tx_date','amount','cost','brand','category'])\n",
    "    return customers, tx\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    customers_df, tx_df = make_synthetic()\n",
    "else:\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        tx_df = spark.table('silver.transactions').toPandas()\n",
    "        customers_df = spark.table('silver.customers').toPandas()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('Implement real data loading or set USE_SYNTHETIC=True') from e\n",
    "\n",
    "customers_df.head(3), tx_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e10fbd",
   "metadata": {},
   "source": [
    "## Anti‑leakage: filter transactions to events on/before AS_OF_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce219ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_df['tx_date'] = pd.to_datetime(tx_df['tx_date'])\n",
    "tx_preT = tx_df[tx_df['tx_date'] <= AS_OF_DATE].copy()\n",
    "post_count = (tx_df['tx_date'] > AS_OF_DATE).sum()\n",
    "print({'post_as_of_transactions_dropped': int(post_count)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38019006",
   "metadata": {},
   "source": [
    "## RFM features anchored at AS_OF_DATE\n",
    "- Recency: days since last purchase before/on AS_OF_DATE\n",
    "- Frequency: count of purchases in last 365 days before AS_OF_DATE\n",
    "- Monetary: sum and average of amount in last 365 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_start = AS_OF_DATE - pd.Timedelta(days=365)\n",
    "tx_12m = tx_preT[(tx_preT['tx_date'] > win_start)]\n",
    "last_tx = tx_preT.groupby('customer_id')['tx_date'].max().rename('last_tx')\n",
    "recency_days = (AS_OF_DATE - last_tx).dt.days.rename('recency_days')\n",
    "freq_12m = tx_12m.groupby('customer_id').size().rename('freq_12m')\n",
    "monetary_sum_12m = tx_12m.groupby('customer_id')['amount'].sum().rename('monetary_sum_12m')\n",
    "monetary_avg_12m = tx_12m.groupby('customer_id')['amount'].mean().rename('monetary_avg_12m')\n",
    "rfm = customers_df[['customer_id','brand']].set_index('customer_id')\n",
    "rfm = rfm.join([recency_days, freq_12m, monetary_sum_12m, monetary_avg_12m]).fillna({\n",
    "    'recency_days': 1e9, 'freq_12m': 0, 'monetary_sum_12m': 0.0, 'monetary_avg_12m': 0.0\n",
    "}).reset_index()\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1ab9b",
   "metadata": {},
   "source": [
    "## Basket diversity and cross‑brand features (last 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea842c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_12m = tx_12m.groupby('customer_id')['category'].nunique().rename('basket_diversity_12m')\n",
    "brand_counts = tx_preT.groupby(['customer_id'])['brand'].nunique().rename('brand_count_all')\n",
    "has_both_brands = (brand_counts >= 2).astype(int).rename('has_both_brands')\n",
    "features = rfm.set_index('customer_id').join([div_12m, has_both_brands]).fillna({'basket_diversity_12m': 0, 'has_both_brands': 0}).reset_index()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ab684",
   "metadata": {},
   "source": [
    "## Screening: correlation, mutual information, and cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['recency_days','freq_12m','monetary_sum_12m','monetary_avg_12m','basket_diversity_12m','has_both_brands']\n",
    "corr = features[num_cols].corr()\n",
    "plt.figure(figsize=(6,4)); sns.heatmap(corr, annot=True, fmt='.2f', cmap='Blues'); plt.title('Correlation heatmap'); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ARTIFACT_DIR, 'corr_heatmap.png'))\n",
    "# Fake target for MI example (replace with real task later), using freq as proxy\n",
    "mi = mutual_info_regression(features[num_cols].fillna(0), features['freq_12m'].fillna(0))\n",
    "mi_series = pd.Series(mi, index=num_cols).sort_values(ascending=False)\n",
    "mi_series.to_csv(os.path.join(ARTIFACT_DIR, 'mutual_information.csv'))\n",
    "card = features.nunique().sort_values(ascending=False)\n",
    "card.to_csv(os.path.join(ARTIFACT_DIR, 'cardinality.csv'))\n",
    "print('Saved heatmap and screening metrics to', ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041eb185",
   "metadata": {},
   "source": [
    "## Train‑only preprocessing (spec)\n",
    "Use splits from Feature 3.1 if available; otherwise, sample a train mask. Fit imputers on TRAIN only and save params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_path = 'artifacts/splits/feature_3_1_splits.csv'\n",
    "if os.path.exists(split_path):\n",
    "    splits = pd.read_csv(split_path)\n",
    "    features = features.merge(splits, on='customer_id', how='left')\n",
    "else:\n",
    "    features['split'] = np.where(np.random.rand(len(features)) < 0.7, 'train', 'test')\n",
    "\n",
    "train_mask = features['split'].fillna('train').eq('train')\n",
    "impute_params = {}\n",
    "for c in num_cols:\n",
    "    mean_val = float(features.loc[train_mask, c].fillna(0).mean())\n",
    "    impute_params[c] = {'strategy': 'mean', 'value': mean_val}\n",
    "with open(os.path.join(ARTIFACT_DIR, 'impute_params.json'), 'w') as f:\n",
    "    json.dump(impute_params, f, indent=2)\n",
    "print('Saved impute params for TRAIN split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dece7ed",
   "metadata": {},
   "source": [
    "## Persist versioned features (Parquet) and attempt Delta if Spark is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['version'] = FEATURE_VERSION\n",
    "features['created_ts'] = pd.Timestamp.utcnow()\n",
    "features['source_snapshot'] = SOURCE_SNAPSHOT\n",
    "features['as_of_date'] = AS_OF_DATE\n",
    "out_parquet = os.path.join(ARTIFACT_DIR, f'customer_features_{FEATURE_VERSION}.parquet')\n",
    "features.to_parquet(out_parquet, index=False)\n",
    "print('Wrote', out_parquet)\n",
    "\n",
    "# Try Spark + Delta write\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    sdf = spark.createDataFrame(features)\n",
    "    # Example managed table write (adjust catalog/schema as needed)\n",
    "    target_table = f'silver.customer_features_{FEATURE_VERSION}'\n",
    "    sdf.write.mode('overwrite').format('delta').saveAsTable(target_table)\n",
    "    print('Saved Delta table', target_table)\n",
    "except Exception as e:\n",
    "    print('Delta write skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab1542",
   "metadata": {},
   "source": [
    "## Log metadata to MLflow (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2044212",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow is not None:\n",
    "    try:\n",
    "        mlflow.set_experiment('/Feature_3_2_Features')\n",
    "        with mlflow.start_run(run_name=f'features_{FEATURE_VERSION}'):\n",
    "            mlflow.log_params({\n",
    "                'version': FEATURE_VERSION,\n",
    "                'as_of_date': str(AS_OF_DATE.date()),\n",
    "                'source_snapshot': SOURCE_SNAPSHOT,\n",
    "                'feature_count': int(features.shape[1])\n",
    "            })\n",
    "            mlflow.log_artifact(os.path.join(ARTIFACT_DIR, 'corr_heatmap.png'))\n",
    "            mlflow.log_artifact(os.path.join(ARTIFACT_DIR, 'mutual_information.csv'))\n",
    "            mlflow.log_artifact(os.path.join(ARTIFACT_DIR, 'cardinality.csv'))\n",
    "            mlflow.log_artifact(os.path.join(ARTIFACT_DIR, 'impute_params.json'))\n",
    "            mlflow.log_artifact(out_parquet)\n",
    "        print('Logged to MLflow')\n",
    "    except Exception as e:\n",
    "        print('MLflow logging skipped:', e)\n",
    "else:\n",
    "    print('MLflow not available')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
