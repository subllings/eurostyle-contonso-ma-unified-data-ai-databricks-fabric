{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2319be76",
   "metadata": {},
   "source": [
    "# Feature 3.4 â€” Batch Scoring & Integration\n",
    "\n",
    "This notebook loads trained models, scores churn probabilities and CLV, runs simple train/serve skew checks, and writes an output table (synthetic fallback). Replace synthetic blocks with Delta/Spark reads and MERGE writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, numpy as np, pandas as pd, random, time, warnings\n",
    "SEED = int(os.getenv('SEED','42'))\n",
    "np.random.seed(SEED); random.seed(SEED)\n",
    "AS_OF_DATE = os.getenv('AS_OF_DATE','2024-06-30')\n",
    "OUT_DIR = os.getenv('OUT_DIR','artifacts/feature_3_4')\n",
    "pathlib.Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print('SEED',SEED,'AS_OF_DATE',AS_OF_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic features and toy models (fallback). Replace with Delta reads and MLflow model loading.\n",
    "def make_features(n=5000, p=20, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n,p))\n",
    "    customers = pd.DataFrame(X, columns=[f'f{i}' for i in range(p)])\n",
    "    customers['customer_id'] = np.arange(n)\n",
    "    return customers\n",
    "\n",
    "features_df = make_features()\n",
    "features = [c for c in features_df.columns if c.startswith('f')]\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe647a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models (here: synthetic random scorers). In production, load from MLflow registry or serialized artifacts.\n",
    "def score_churn(X):\n",
    "    # Stub: random but smoothed probabilities\n",
    "    p = 1/(1+np.exp(-0.2*X[:,0] + 0.1*X[:,1]))\n",
    "    return 0.9*p + 0.1*0.2\n",
    "def score_clv(X):\n",
    "    # Stub: positive values\n",
    "    return np.maximum(0, 200 + 30*X[:,0] + 20*X[:,1])\n",
    "\n",
    "X = features_df[features].values\n",
    "churn_score = score_churn(X)\n",
    "clv_pred = score_clv(X)\n",
    "print(churn_score.min(), churn_score.max(), clv_pred.min(), clv_pred.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1eda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing + basic QA\n",
    "def decile_bucket(arr):\n",
    "    q = np.nanpercentile(arr, np.arange(0,110,10))\n",
    "    return np.clip(np.digitize(arr, q[1:-1], right=True)+1, 1, 10)\n",
    "features_df['churn_score'] = churn_score\n",
    "features_df['churn_bucket'] = decile_bucket(churn_score)\n",
    "features_df['clv_pred'] = clv_pred\n",
    "features_df['model_version'] = os.getenv('MODEL_VERSION','v1')\n",
    "features_df['feature_version'] = os.getenv('FEATURE_VERSION','v1')\n",
    "features_df['as_of_date'] = AS_OF_DATE\n",
    "features_df['scored_ts'] = pd.Timestamp.utcnow()\n",
    "# QA checks\n",
    "assert features_df['churn_score'].between(0,1).all()\n",
    "assert (features_df['clv_pred']>=0).all()\n",
    "assert features_df['customer_id'].is_unique\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/serve skew check (toy): compare to a stored reference mean/std (synthetic).\n",
    "ref = pd.DataFrame({'feature':features, 'mean_ref':0.0, 'std_ref':1.0})\n",
    "cur = features_df[features].mean().reset_index(); cur.columns=['feature','mean_cur']\n",
    "skew = ref.merge(cur, on='feature', how='left')\n",
    "skew['delta_mean'] = (skew['mean_cur']-skew['mean_ref']).abs()\n",
    "skew.to_csv(f'{OUT_DIR}/skew_report.csv', index=False)\n",
    "skew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist scored output (CSV fallback). In Databricks, write Delta and MERGE/overwrite-by-partition.\n",
    "out = features_df[['customer_id','churn_score','churn_bucket','clv_pred','model_version','feature_version','as_of_date','scored_ts']]\n",
    "out.to_csv(f'{OUT_DIR}/customer_scores_gold.csv', index=False)\n",
    "print('Saved', len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773ecb0",
   "metadata": {},
   "source": [
    "Join to `customer_360_gold` by `customer_id` in your warehouse/lakehouse. Ensure counts/keys match, and publish the schema contract to analysts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
