{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac728e21",
   "metadata": {},
   "source": [
    "# Feature 3.1 — Exploratory Data Analysis (EDA)\n",
    "This notebook implements the acceptance criteria and tasks for Feature 3.1 (EDA): profiling, churn definition (>90 days inactivity), CLV draft (12-month net margin), leakage guardrails, non-leaky splits, naive baselines, MLflow logging, and drift/overlap checks between brands (Contoso vs EuroStyle).\n",
    "\n",
    "Notes:\n",
    "- Uses a synthetic fallback dataset so the notebook runs end-to-end without data access.\n",
    "- Replace placeholders with your actual table names/paths and set `USE_SYNTHETIC = False` when connecting to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaeda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os, sys, math, json, warnings, random\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "try:\n",
    "    import mlflow\n",
    "except Exception as e:\n",
    "    mlflow = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e334e4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "- Freeze seed for reproducibility.\n",
    "- Record dataset snapshot and churn horizon.\n",
    "- Set MLflow experiment (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "DATASET_SNAPSHOT = os.environ.get('DATASET_SNAPSHOT', datetime.utcnow().strftime('%Y-%m-%d'))\n",
    "CHURN_HORIZON_DAYS = int(os.environ.get('CHURN_HORIZON_DAYS', '90'))\n",
    "CUTOFF_DATE = pd.to_datetime(os.environ.get('CUTOFF_DATE', '2024-12-31'))\n",
    "USE_SYNTHETIC = os.environ.get('USE_SYNTHETIC', 'true').lower() in ('1','true','yes')\n",
    "\n",
    "# MLflow setup (optional)\n",
    "MLFLOW_EXPERIMENT = os.environ.get('MLFLOW_EXPERIMENT', '/Feature_3_1_EDA')\n",
    "if mlflow is not None:\n",
    "    try:\n",
    "        mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "    except Exception as e:\n",
    "        print('MLflow not configured:', e)\n",
    "\n",
    "print({'seed': SEED, 'dataset_snapshot': DATASET_SNAPSHOT, 'churn_horizon_days': CHURN_HORIZON_DAYS, 'cutoff_date': str(CUTOFF_DATE), 'use_synthetic': USE_SYNTHETIC})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0886723c",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Replace placeholders with real tables/paths. Expected columns (minimum):\n",
    "- customers: customer_id, brand\n",
    "- transactions: customer_id, tx_date, amount, cost, brand\n",
    "If `USE_SYNTHETIC=True`, a small dataset will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic(n_customers=1000, start='2023-01-01', end='2024-12-31'):\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    customers = pd.DataFrame({\n",
    "        'customer_id': np.arange(1, n_customers+1),\n",
    "        'brand': rng.choice(['Contoso','EuroStyle'], size=n_customers, p=[0.6,0.4])\n",
    "    })\n",
    "    dates = pd.date_range(start=start, end=end, freq='D')\n",
    "    rows = []\n",
    "    for cid, brand in customers[['customer_id','brand']].itertuples(index=False):\n",
    "        k = rng.poisson(10)\n",
    "        if k == 0: continue\n",
    "        tx_days = rng.choice(dates, size=k, replace=False)\n",
    "        tx_days.sort()\n",
    "        for d in tx_days:\n",
    "            amount = float(np.round(rng.normal(60 if brand=='Contoso' else 55, 20), 2))\n",
    "            amount = max(5.0, amount)\n",
    "            cost = float(np.round(amount * rng.uniform(0.5, 0.8), 2))\n",
    "            rows.append((cid, pd.Timestamp(d), amount, cost, brand))\n",
    "    tx = pd.DataFrame(rows, columns=['customer_id','tx_date','amount','cost','brand'])\n",
    "    return customers, tx\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    customers_df, tx_df = make_synthetic()\n",
    "else:\n",
    "    # TODO: Replace with actual load, e.g., Spark or pandas from Lakehouse/Delta\n",
    "    # from pyspark.sql import SparkSession\n",
    "    # spark = SparkSession.builder.getOrCreate()\n",
    "    # tx_df = spark.table('bronze.transactions').toPandas()\n",
    "    # customers_df = spark.table('bronze.customers').toPandas()\n",
    "    raise RuntimeError('Set USE_SYNTHETIC=True or implement real data load')\n",
    "\n",
    "customers_df.head(), tx_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf91eb",
   "metadata": {},
   "source": [
    "## Quick profiling\n",
    "Shapes, missingness, distributions, outliers, correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('customers_df shape:', customers_df.shape)\n",
    "print('tx_df shape:', tx_df.shape)\n",
    "display(customers_df.head(3))\n",
    "display(tx_df.head(3))\n",
    "\n",
    "# Missingness\n",
    "missing_tx = tx_df.isna().mean().sort_values(ascending=False)\n",
    "missing_cu = customers_df.isna().mean().sort_values(ascending=False)\n",
    "print('Missing (transactions): ', missing_tx)\n",
    "print('Missing (customers): ', missing_cu)\n",
    "\n",
    "# Distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "sns.histplot(tx_df['amount'], ax=axes[0], kde=True)\n",
    "axes[0].set_title('Amount distribution')\n",
    "tx_per_cust = tx_df.groupby('customer_id').size()\n",
    "sns.histplot(tx_per_cust, ax=axes[1], kde=False)\n",
    "axes[1].set_title('Transactions per customer')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Outliers via IQR\n",
    "q1, q3 = tx_df['amount'].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "upper = q3 + 1.5*iqr\n",
    "outlier_rate = (tx_df['amount'] > upper).mean()\n",
    "print({'amount_outlier_rate': float(outlier_rate), 'upper_whisker': float(upper)})\n",
    "\n",
    "# Correlation\n",
    "num_cols = tx_df.select_dtypes(include=[np.number]).columns\n",
    "if len(num_cols) > 1:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(tx_df[num_cols].corr(), annot=False, cmap='Blues')\n",
    "    plt.title('Numeric correlation (transactions)'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c988d1",
   "metadata": {},
   "source": [
    "## Churn label (> 90 days inactivity) and prevalence\n",
    "Compute last activity per customer relative to CUTOFF_DATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_df['tx_date'] = pd.to_datetime(tx_df['tx_date'])\n",
    "last_tx = tx_df.groupby('customer_id')['tx_date'].max().rename('last_activity_date')\n",
    "cust = customers_df.merge(last_tx, on='customer_id', how='left')\n",
    "cust['days_inactive'] = (CUTOFF_DATE - cust['last_activity_date']).dt.days\n",
    "cust['churn_90d'] = cust['days_inactive'] > CHURN_HORIZON_DAYS\n",
    "\n",
    "prevalence = cust['churn_90d'].mean()\n",
    "prev_by_brand = cust.groupby('brand')['churn_90d'].mean().sort_values(ascending=False)\n",
    "print({'churn_prevalence_overall': float(prevalence)})\n",
    "print('churn_prevalence_by_brand:\n",
    "', prev_by_brand)\n",
    "cust.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd88018",
   "metadata": {},
   "source": [
    "## CLV (12-month net margin) — draft\n",
    "Draft a simple CLV proxy: past 12-month net margin as a baseline (can be refined later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_start = CUTOFF_DATE - pd.Timedelta(days=365)\n",
    "tx_12m = tx_df[(tx_df['tx_date'] > horizon_start) & (tx_df['tx_date'] <= CUTOFF_DATE)].copy()\n",
    "tx_12m['margin'] = tx_12m['amount'] - tx_12m['cost']\n",
    "clv = tx_12m.groupby('customer_id')['margin'].sum().rename('clv_12m_margin')\n",
    "cust = cust.merge(clv, on='customer_id', how='left').fillna({'clv_12m_margin': 0.0})\n",
    "cust[['customer_id','brand','clv_12m_margin','churn_90d']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17c78d",
   "metadata": {},
   "source": [
    "## Leakage checklist\n",
    "Flag fields with potential future information relative to CUTOFF_DATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_leak_cols = [c for c in tx_df.columns if 'cancel' in c.lower() or 'refund' in c.lower() or 'return' in c.lower()]\n",
    "date_cols = [c for c in tx_df.columns if 'date' in c.lower()]\n",
    "post_cutoff_flags = {}\n",
    "for c in date_cols:\n",
    "    try:\n",
    "        post_cutoff_flags[c] = bool((pd.to_datetime(tx_df[c]) > CUTOFF_DATE).any())\n",
    "    except Exception:\n",
    "        post_cutoff_flags[c] = False\n",
    "print({'leakage_keywords': potential_leak_cols, 'post_cutoff_date_cols': post_cutoff_flags})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248350a4",
   "metadata": {},
   "source": [
    "## Non‑leaky splits\n",
    "Use a time-based holdout and, optionally, GroupKFold by customer. Persist artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e960183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split: train until CUTOFF_DATE - 90d, validate next 45d, test next 45d (example)\n",
    "train_end = CUTOFF_DATE - pd.Timedelta(days=90)\n",
    "val_end = train_end + pd.Timedelta(days=45)\n",
    "\n",
    "cust_dates = tx_df.groupby('customer_id')['tx_date'].max().rename('last_tx')\n",
    "cust_split = customers_df[['customer_id','brand']].merge(cust_dates, on='customer_id', how='left')\n",
    "cust_split['split'] = np.where(cust_split['last_tx'] <= train_end, 'train',\n",
    "                           np.where(cust_split['last_tx'] <= val_end, 'val', 'test'))\n",
    "cust_split['split'].value_counts(dropna=False)\n",
    "\n",
    "# Persist artifacts\n",
    "os.makedirs('artifacts/splits', exist_ok=True)\n",
    "cust_split[['customer_id','split']].to_csv('artifacts/splits/feature_3_1_splits.csv', index=False)\n",
    "with open('artifacts/splits/feature_3_1_split_meta.json','w') as f:\n",
    "    json.dump({'seed': SEED, 'cutoff_date': str(CUTOFF_DATE), 'train_end': str(train_end), 'val_end': str(val_end)}, f, indent=2)\n",
    "cust_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253954ea",
   "metadata": {},
   "source": [
    "## Baselines: Rule-based churn and RFM\n",
    "Compute simple yardsticks and log to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based churn baseline: predict churn if days_inactive > THRESH\n",
    "THRESH = CHURN_HORIZON_DAYS\n",
    "pred_score = (cust['days_inactive'] / (THRESH + 1)).clip(0, 1.5)\n",
    "y_true = cust['churn_90d'].astype(int)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, pred_score)\n",
    "    ap = average_precision_score(y_true, pred_score)\n",
    "except Exception:\n",
    "    auc = float('nan'); ap = float('nan')\n",
    "print({'rule_auc': float(auc), 'rule_pr_auc': float(ap)})\n",
    "\n",
    "# RFM bins (simple)\n",
    "recency = (CUTOFF_DATE - cust['last_activity_date']).dt.days.fillna(1e9)\n",
    "frequency = tx_df.groupby('customer_id').size().reindex(cust['customer_id']).fillna(0)\n",
    "monetary = tx_df.groupby('customer_id')['amount'].sum().reindex(cust['customer_id']).fillna(0)\n",
    "cust['R_bin'] = pd.qcut(recency, q=3, labels=['R3','R2','R1'])\n",
    "cust['F_bin'] = pd.qcut(frequency.rank(method='first'), q=3, labels=['F1','F2','F3'])\n",
    "cust['M_bin'] = pd.qcut(monetary.rank(method='first'), q=3, labels=['M1','M2','M3'])\n",
    "cust['RFM'] = cust['R_bin'].astype(str) + cust['F_bin'].astype(str) + cust['M_bin'].astype(str)\n",
    "rfm_dist = cust['RFM'].value_counts().head(10)\n",
    "print('Top RFM bins:', rfm_dist)\n",
    "\n",
    "# Log to MLflow\n",
    "if mlflow is not None:\n",
    "    try:\n",
    "        with mlflow.start_run(run_name='feature_3_1_baselines'):\n",
    "            mlflow.log_params({'seed': SEED, 'churn_horizon_days': CHURN_HORIZON_DAYS, 'cutoff_date': str(CUTOFF_DATE)})\n",
    "            mlflow.log_metrics({'rule_auc': float(auc), 'rule_pr_auc': float(ap)})\n",
    "            # Save small artifacts\n",
    "            os.makedirs('artifacts/baselines', exist_ok=True)\n",
    "            rfm_path = 'artifacts/baselines/rfm_top10.csv'\n",
    "            rfm_dist.to_csv(rfm_path)\n",
    "            mlflow.log_artifact(rfm_path)\n",
    "    except Exception as e:\n",
    "        print('MLflow logging skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51cb2f2",
   "metadata": {},
   "source": [
    "## Drift and brand overlaps (EuroStyle vs Contoso)\n",
    "Compare feature distributions across brands using KS test or Population Stability Index (PSI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def psi(expected, actual, buckets=10, eps=1e-6):\n",
    "    qs = np.linspace(0, 1, buckets+1)\n",
    "    e_bins = np.quantile(expected, qs)\n",
    "    a_bins = e_bins  # use expected bins\n",
    "    e_counts, _ = np.histogram(expected, bins=e_bins)\n",
    "    a_counts, _ = np.histogram(actual, bins=a_bins)\n",
    "    e_pct = e_counts / (e_counts.sum() + eps)\n",
    "    a_pct = a_counts / (a_counts.sum() + eps)\n",
    "    val = ((a_pct - e_pct) * np.log((a_pct + eps) / (e_pct + eps))).sum()\n",
    "    return float(val)\n",
    "\n",
    "numerics = ['amount']\n",
    "res = []\n",
    "for col in numerics:\n",
    "    a = tx_df.loc[tx_df['brand']=='Contoso', col].dropna()\n",
    "    b = tx_df.loc[tx_df['brand']=='EuroStyle', col].dropna()\n",
    "    if len(a) > 10 and len(b) > 10:\n",
    "        ks = ks_2samp(a, b).statistic\n",
    "        psi_val = psi(a.values, b.values)\n",
    "        res.append({'feature': col, 'ks': float(ks), 'psi': float(psi_val)})\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94753ed",
   "metadata": {},
   "source": [
    "## Save EDA artifacts and summary\n",
    "Outputs: split artifacts, top issues (placeholder), and basic README text for the readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('artifacts/eda', exist_ok=True)\n",
    "# Example top issues placeholder (replace during real EDA)\n",
    "top_issues = [\n",
    "    {'issue': 'Check missing last_activity_date for some customers', 'severity': 'medium', 'owner': 'DE'},\n",
    "    {'issue': 'Outliers in amount distribution (upper whisker exceeded)', 'severity': 'low', 'owner': 'DA'},\n",
    "    {'issue': 'Clarify cost vs margin fields semantics', 'severity': 'high', 'owner': 'DA'}\n",
    "]\n",
    "pd.DataFrame(top_issues).to_csv('artifacts/eda/top_issues.csv', index=False)\n",
    "with open('artifacts/eda/README_feature_3_1_summary.txt','w') as f:\n",
    "    f.write(f\"Dataset snapshot: {DATASET_SNAPSHOT}\\nCutoff date: {CUTOFF_DATE.date()}\\nChurn horizon: {CHURN_HORIZON_DAYS} days\\nBaseline AUC (rule): {auc:.3f} | PR-AUC: {ap:.3f}\\n\\nSee artifacts/splits and artifacts/baselines.\\n\")\n",
    "print('Saved artifacts under artifacts/eda and artifacts/splits/baselines')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
